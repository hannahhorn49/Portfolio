{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Programmer: Hannah Horn\n",
    "# Class: CPSC 322-01 Fall 2024\n",
    "# Programming Assignment #7\n",
    "# 11/20/24\n",
    "# I did not attempt the bonus\n",
    "# Description: This program determines how well four different \n",
    "# classifiers (KNN, Naive Bayes, Dummy, and Decision Tree) classify the winner\n",
    "# of the March Madness Tournament based on various attributes.\n",
    "# Various evaluation metrics are used. \n",
    "##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful mysklearn package import statements and reloads\n",
    "import importlib\n",
    "\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "# uncomment once you paste your mypytable.py into mysklearn package\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "# uncomment once you paste your myclassifiers.py into mysklearn package\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, MyDummyClassifier, MyNaiveBayesClassifier, MyDecisionTreeClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: üèÄ Basketball Winner Classification üèÄs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part will use the various classifiers: kNN, Dummy, Naive Bayes, and a Decision Tree to see how well each of them predict the winner of the March Madness Tournament. \n",
    "\n",
    "Before performing classification, we removed the attributes (\"Season\", \"HomeTeam\", and \"AwayTeam\") since they were too specific to use as features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Using only the TournamentSeed Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we only train the model using the TournamentSeed feature which provides a baseline set of results. \n",
    "\n",
    "In this step, after chosing the TournamentSeed as our X and Winner as our y, we create a dummy, kNN, Naive Bayes, and decision tree classifier and test each using a k-fold cross-validation. We then compare how well each classifier performs by comparing the accuracy and error rate, precision, recall, f1, and the confusion matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "10-Fold Cross Validation Results for KNN Classifier\n",
      "========================================================\n",
      "Accuracy = 0.48, error rate = 0.52\n",
      "Precision = 0.00\n",
      "Recall = 0.00\n",
      "F1 Measure = 0.00\n",
      "\n",
      "Confusion Matrix:\n",
      "Actual Class      A    H    Total    Recognition (%)\n",
      "--------------  ---  ---  -------  -----------------\n",
      "A               159    0      159              100.0\n",
      "H               175    0      175                0.0\n"
     ]
    }
   ],
   "source": [
    "X, y = myutils.prepare_categorical_data_random_subsamples()\n",
    "knn_classifier = MyKNeighborsClassifier(n_neighbors = 3, categorical = [True, True])\n",
    "\n",
    "print(\"========================================================\")\n",
    "print(\"10-Fold Cross Validation Results for KNN Classifier\")\n",
    "print(\"========================================================\")\n",
    "results = myutils.evaluate_classifier(X, y, knn_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "10-Fold Cross Validation Results for Dummy Classifier\n",
      "========================================================\n",
      "Accuracy = 0.52, error rate = 0.48\n",
      "Precision = 0.52\n",
      "Recall = 1.00\n",
      "F1 Measure = 0.69\n",
      "\n",
      "Confusion Matrix:\n",
      "Actual Class      A    H    Total    Recognition (%)\n",
      "--------------  ---  ---  -------  -----------------\n",
      "A                 0  159      159                0.0\n",
      "H                 0  175      175              100.0\n"
     ]
    }
   ],
   "source": [
    "X, y = myutils.prepare_categorical_data_random_subsamples()\n",
    "dummy_classifier = MyDummyClassifier()\n",
    "\n",
    "print(\"========================================================\")\n",
    "print(\"10-Fold Cross Validation Results for Dummy Classifier\")\n",
    "print(\"========================================================\")\n",
    "results = myutils.evaluate_classifier(X, y, dummy_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "10-Fold Cross Validation Results for Naive Bayes Classifier\n",
      "========================================================\n",
      "Accuracy = 0.69, error rate = 0.31\n",
      "Precision = 0.68\n",
      "Recall = 0.76\n",
      "F1 Measure = 0.72\n",
      "\n",
      "Confusion Matrix:\n",
      "Actual Class      A    H    Total    Recognition (%)\n",
      "--------------  ---  ---  -------  -----------------\n",
      "A                97   62      159               61.0\n",
      "H                42  133      175               76.0\n"
     ]
    }
   ],
   "source": [
    "X, y = myutils.prepare_categorical_data_random_subsamples()\n",
    "naive_classifier = MyNaiveBayesClassifier()\n",
    "\n",
    "print(\"========================================================\")\n",
    "print(\"10-Fold Cross Validation Results for Naive Bayes Classifier\")\n",
    "print(\"========================================================\")\n",
    "results = myutils.evaluate_classifier(X, y, naive_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================\n",
      "10-Fold Cross Validation Results for Decision Tree Classifier\n",
      "=============================================================\n",
      "Accuracy = 0.69, error rate = 0.31\n",
      "Precision = 0.68\n",
      "Recall = 0.76\n",
      "F1 Measure = 0.72\n",
      "\n",
      "Confusion Matrix:\n",
      "Actual Class      A    H    Total    Recognition (%)\n",
      "--------------  ---  ---  -------  -----------------\n",
      "A                97   62      159               61.0\n",
      "H                42  133      175               76.0\n"
     ]
    }
   ],
   "source": [
    "X, y = myutils.prepare_categorical_data_random_subsamples()\n",
    "decision_tree_classifier = MyDecisionTreeClassifier()\n",
    "\n",
    "print(\"==============================================================\")\n",
    "print(\"10-Fold Cross Validation Results for Decision Tree Classifier\")\n",
    "print(\"=============================================================\")\n",
    "results = myutils.evaluate_classifier(X, y, decision_tree_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Using a Feature Subset of your Choosing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we explored different features to determine which combinations were better at predicting the Winner compared to our baseline of just the TournamentSeed. \n",
    "\n",
    "Since Decision Trees tend to overfit the data, combinations of 2-4 attributes were looked at. Through this exploration, it was difficult to find attribute combinations that did signifcantly better than just TournamentSeed alone. However, it seemed that the `combination of a teams FG%, FG3%, and their ordinal rank` had a very balanced result in how well it recognized each class. \n",
    "\n",
    "Similar to the last step, we then compare how well each classifier performs by comparing the accuracy and error rate, precision, recall, f1, and the confusion matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "10-Fold Cross Validation Results for KNN Classifier\n",
      "========================================================\n",
      "Accuracy = 0.63, error rate = 0.37\n",
      "Precision = 0.69\n",
      "Recall = 0.53\n",
      "F1 Measure = 0.60\n",
      "\n",
      "Confusion Matrix:\n",
      "Actual Class      A    H    Total    Recognition (%)\n",
      "--------------  ---  ---  -------  -----------------\n",
      "A               118   41      159               74.2\n",
      "H                82   93      175               53.1\n"
     ]
    }
   ],
   "source": [
    "columns_to_select = [\"RegularSeasonFGPercentMean\", \"RegularSeasonFG3PercentMean\", \"LastOrdinalRank\"]\n",
    "X_train, y_train = myutils.prepare_categorical_data_with_selected_columns(columns_to_select)\n",
    "\n",
    "knn_classifier = MyKNeighborsClassifier(n_neighbors = 3, categorical = [True, True, True])\n",
    "\n",
    "print(\"========================================================\")\n",
    "print(\"10-Fold Cross Validation Results for KNN Classifier\")\n",
    "print(\"========================================================\")\n",
    "results = myutils.evaluate_classifier(X_train, y_train, knn_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "10-Fold Cross Validation Results for Dummy Classifier\n",
      "========================================================\n",
      "Accuracy = 0.52, error rate = 0.48\n",
      "Precision = 0.52\n",
      "Recall = 1.00\n",
      "F1 Measure = 0.69\n",
      "\n",
      "Confusion Matrix:\n",
      "Actual Class      A    H    Total    Recognition (%)\n",
      "--------------  ---  ---  -------  -----------------\n",
      "A                 0  159      159                0.0\n",
      "H                 0  175      175              100.0\n"
     ]
    }
   ],
   "source": [
    "columns_to_select = [\"RegularSeasonFGPercentMean\", \"RegularSeasonFG3PercentMean\", \"LastOrdinalRank\"]\n",
    "X_train, y_train = myutils.prepare_categorical_data_with_selected_columns(columns_to_select)\n",
    "\n",
    "dummy_classifier = MyDummyClassifier()\n",
    "\n",
    "print(\"========================================================\")\n",
    "print(\"10-Fold Cross Validation Results for Dummy Classifier\")\n",
    "print(\"========================================================\")\n",
    "results = myutils.evaluate_classifier(X_train, y_train, dummy_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "10-Fold Cross Validation Results for Naive Bayes Classifier\n",
      "========================================================\n",
      "Accuracy = 0.68, error rate = 0.32\n",
      "Precision = 0.69\n",
      "Recall = 0.70\n",
      "F1 Measure = 0.70\n",
      "\n",
      "Confusion Matrix:\n",
      "Actual Class      A    H    Total    Recognition (%)\n",
      "--------------  ---  ---  -------  -----------------\n",
      "A               104   55      159               65.4\n",
      "H                52  123      175               70.3\n"
     ]
    }
   ],
   "source": [
    "columns_to_select = [\"RegularSeasonFGPercentMean\", \"RegularSeasonFG3PercentMean\", \"LastOrdinalRank\"]\n",
    "X_train, y_train = myutils.prepare_categorical_data_with_selected_columns(columns_to_select)\n",
    "\n",
    "naive_classifier = MyNaiveBayesClassifier()\n",
    "\n",
    "print(\"========================================================\")\n",
    "print(\"10-Fold Cross Validation Results for Naive Bayes Classifier\")\n",
    "print(\"========================================================\")\n",
    "results = myutils.evaluate_classifier(X_train, y_train, naive_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "Decision Tree Results- Trained Over Whole Dataset\n",
      "========================================================\n",
      "Accuracy = 0.69, error rate = 0.31\n",
      "Precision = 0.71\n",
      "Recall = 0.70\n",
      "F1 Measure = 0.70\n",
      "\n",
      "Confusion Matrix:\n",
      "Actual Class      A    H    Total    Recognition (%)\n",
      "--------------  ---  ---  -------  -----------------\n",
      "A               108   51      159               67.9\n",
      "H                53  122      175               69.7\n",
      "=========================================================\n",
      "Decision Rules Inferred by the Decision Tree Classifier\n",
      "=========================================================\n",
      "IF LastOrdinalRank == A AND RegularSeasonFGPercentMean == A AND RegularSeasonFG3PercentMean == A THEN Winner = H\n",
      "IF LastOrdinalRank == A AND RegularSeasonFGPercentMean == A AND RegularSeasonFG3PercentMean == H THEN Winner = H\n",
      "IF LastOrdinalRank == A AND RegularSeasonFGPercentMean == H AND RegularSeasonFG3PercentMean == A THEN Winner = H\n",
      "IF LastOrdinalRank == A AND RegularSeasonFGPercentMean == H AND RegularSeasonFG3PercentMean == H THEN Winner = H\n",
      "IF LastOrdinalRank == H AND RegularSeasonFG3PercentMean == A AND RegularSeasonFGPercentMean == A THEN Winner = A\n",
      "IF LastOrdinalRank == H AND RegularSeasonFG3PercentMean == A AND RegularSeasonFGPercentMean == H THEN Winner = A\n",
      "IF LastOrdinalRank == H AND RegularSeasonFG3PercentMean == H AND RegularSeasonFGPercentMean == A THEN Winner = A\n",
      "IF LastOrdinalRank == H AND RegularSeasonFG3PercentMean == H AND RegularSeasonFGPercentMean == H THEN Winner = A\n"
     ]
    }
   ],
   "source": [
    "decision_tree_classifier = MyDecisionTreeClassifier()\n",
    "results = myutils.evaluate_tree_classifier(decision_tree_classifier)\n",
    "\n",
    "attribute_names = columns_to_select  # or a custom list of feature names\n",
    "print(\"=========================================================\")\n",
    "print(\"Decision Rules Inferred by the Decision Tree Classifier\")\n",
    "print(\"=========================================================\")\n",
    "decision_tree_classifier.print_decision_rules(attribute_names=attribute_names, class_name=\"Winner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree Pruning Analysis:\n",
    "\n",
    "Pruning a tree involves simplifying the tree by removing unncessary branches while retaining the ability to make accurate predictions. In this case, we could prune our tree to just include the **LastOrdinalRank** attribute. This is because when LastOrdinalRank == A, the outcome is consistently predicted as Winner = H regardless of the values of RegularSeasonFGPercentMean and RegularSeasonFG3Percent Mean. This is the same as when  LastOrdinalRank == H, the outcome is consistently predicted as Winner = A. \n",
    "\n",
    "We can infer that it seems that the values of RegularSeasonFGPercentMean and RegularSeasonFG3Percent Mean do not influence the decision when the LastOrdinalRank is known. Since they are unncessary for the classification outcome, they can be pruned. \n",
    "\n",
    "As a result, the pruned decision rules are:\n",
    "IF LastOrdinalRank == A THEN Winner = H\n",
    "IF LastOrdinalRank == H THEN Winner = A"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
